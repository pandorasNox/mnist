{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0.dev20181207\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# %pylab inline\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "print(torch.__version__)\n",
    "print(\"0.4.0\" == torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kwargs = {}\n",
    "\n",
    "train = dataloader(\n",
    "    MNIST(\n",
    "        'data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download mnsit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get/load trainigs data\n",
    "\n",
    "train_data = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get/load test data\n",
    "\n",
    "test_data = MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "test_data: Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "# output test & train data\n",
    "\n",
    "print(\"train_data:\", train_data)\n",
    "print(\"\")\n",
    "print(\"test_data:\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================\n",
      "\n",
      "train_data.data.cpu().numpy() [[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=64, num_workers=1, pin_memory=True)\n",
    "train_loader = dataloader.DataLoader(train_data, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test_data, **dataloader_args)\n",
    "\n",
    "print(\"\")\n",
    "print(\"=======================\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"train_data.data.cpu().numpy()\", train_data.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]\n",
      " - Numpy Shape: (60000, 28, 28)\n",
      " - Tensor Shape: torch.Size([60000, 28, 28])\n",
      " - Transformed Shape: torch.Size([28, 60000, 28])\n",
      " - min: tensor(0.)\n",
      " - max: tensor(1.)\n",
      " - mean: tensor(0.1305)\n",
      " - std: tensor(0.3081)\n",
      " - var: tensor(0.0949)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_data = train_data.data\n",
    "transformed_train_data = train_data.transform(train_data.data.cpu().numpy())\n",
    "\n",
    "print('[Train]')\n",
    "print(' - Numpy Shape:', train_data.data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', train_data.data.size())\n",
    "print(' - Transformed Shape:', transformed_train_data.size())\n",
    "print(' - min:', torch.min(transformed_train_data))\n",
    "print(' - max:', torch.max(transformed_train_data))\n",
    "print(' - mean:', torch.mean(transformed_train_data))\n",
    "print(' - std:', torch.std(transformed_train_data))\n",
    "print(' - var:', torch.var(transformed_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOptimizer(net):\n",
    "    return optim.SGD(net.parameters(), lr=0.001, momentum=0.8)\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_epoch_train(model, current_epoch, train_data_loader, optimizer):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    #model.eval()\n",
    "\n",
    "    for batch_id, (data, target) in enumerate(train_data_loader):\n",
    "        #if torch.cuda.is_available():\n",
    "        #if cuda #<= if gpu support, do something like\n",
    "            #data = data.cuda()\n",
    "            #target = target.cuda()\n",
    "            \n",
    "        #transform to variables\n",
    "        data = Variable(data)\n",
    "        target = Variable(target)\n",
    "        \n",
    "        #optimizer = createOptimizer(model) #<= not needed bec is passed\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data) #out OR prediction\n",
    "        #criterion = nn.CrossEntropyLoss\n",
    "        criterion = F.nll_loss # criterion-fn\n",
    "        loss = criterion(out, target)\n",
    "            \n",
    "        batch_losses.append(loss.data)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                current_epoch, \n",
    "                batch_id * len(data), \n",
    "                len(train_data_loader.dataset),\n",
    "                100. * batch_id / len(train_data_loader), \n",
    "                loss.data\n",
    "            ),\n",
    "            end=''\n",
    "        )\n",
    "        \n",
    "    return (model, batch_losses)\n",
    "\n",
    "def train(net, train_data_loader, optimizer, count_epochs=15):\n",
    "    #check count_epochs >= 1\n",
    "    losses = []\n",
    "\n",
    "    for current_epoch in range(1, count_epochs):\n",
    "        (returnedNet, batch_losses) = single_epoch_train(net, current_epoch, train_data_loader, optimizer)\n",
    "        net = returnedNet\n",
    "        losses += batch_losses\n",
    "\n",
    "    return (net, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        \n",
    "        #add layers\n",
    "        #input_features = 10\n",
    "        conv1_inChannels = 1\n",
    "        conv1_outChannels = 10\n",
    "        conv1_kernalSize = 5\n",
    "        self.conv1 = nn.Conv2d(conv1_inChannels, conv1_outChannels, conv1_kernalSize)\n",
    "        \n",
    "        conv2_inChannels = conv1_outChannels\n",
    "        conv2_outChannels = 20\n",
    "        conv2_kernalSize = 5\n",
    "        self.conv2 = nn.Conv2d(conv2_inChannels, conv2_outChannels, conv2_kernalSize)\n",
    "        \n",
    "        self.conv_dropout = nn.Dropout2d\n",
    "        \n",
    "        self.fc1 = nn.Linear(320, 60) #fc1 = fully connected layer #320 = amount of pixels per image\n",
    "        self.fc2 = nn.Linear(60, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #conv1\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #conv2        \n",
    "        x = self.conv2(x)\n",
    "        #x = self.conv_dropout(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #step to determine fc1 inputs??? = result leads to 320, but why, how to calc that???\n",
    "        #print(x.size())\n",
    "        #exit()\n",
    "        \n",
    "        #why???\n",
    "        x = x.view(-1, 320)\n",
    "        \n",
    "        #fc1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #fc2 ~ output\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # helper function ??? why ???\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # attention: mini batch / stochachistig grad\n",
    "        num = 1\n",
    "        for i in size:\n",
    "            num *= i\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 4 [29984/60000 (100%)]\tLoss: 0.189448"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MNISTNet(\n",
       "   (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "   (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "   (fc1): Linear(in_features=320, out_features=60, bias=True)\n",
       "   (fc2): Linear(in_features=60, out_features=10, bias=True)\n",
       " ),\n",
       " [tensor(2.3027),\n",
       "  tensor(2.3253),\n",
       "  tensor(2.3065),\n",
       "  tensor(2.3048),\n",
       "  tensor(2.3078),\n",
       "  tensor(2.3146),\n",
       "  tensor(2.2913),\n",
       "  tensor(2.2932),\n",
       "  tensor(2.2933),\n",
       "  tensor(2.3328),\n",
       "  tensor(2.2995),\n",
       "  tensor(2.3068),\n",
       "  tensor(2.3117),\n",
       "  tensor(2.3012),\n",
       "  tensor(2.2913),\n",
       "  tensor(2.3191),\n",
       "  tensor(2.3119),\n",
       "  tensor(2.3035),\n",
       "  tensor(2.2772),\n",
       "  tensor(2.2947),\n",
       "  tensor(2.3217),\n",
       "  tensor(2.3014),\n",
       "  tensor(2.2982),\n",
       "  tensor(2.2952),\n",
       "  tensor(2.2913),\n",
       "  tensor(2.3106),\n",
       "  tensor(2.3019),\n",
       "  tensor(2.2974),\n",
       "  tensor(2.2915),\n",
       "  tensor(2.3052),\n",
       "  tensor(2.3040),\n",
       "  tensor(2.3137),\n",
       "  tensor(2.3072),\n",
       "  tensor(2.3229),\n",
       "  tensor(2.3256),\n",
       "  tensor(2.3040),\n",
       "  tensor(2.2999),\n",
       "  tensor(2.3254),\n",
       "  tensor(2.3073),\n",
       "  tensor(2.3112),\n",
       "  tensor(2.3169),\n",
       "  tensor(2.3152),\n",
       "  tensor(2.2970),\n",
       "  tensor(2.3043),\n",
       "  tensor(2.3161),\n",
       "  tensor(2.2924),\n",
       "  tensor(2.3040),\n",
       "  tensor(2.2995),\n",
       "  tensor(2.3021),\n",
       "  tensor(2.3103),\n",
       "  tensor(2.3011),\n",
       "  tensor(2.2810),\n",
       "  tensor(2.3152),\n",
       "  tensor(2.2939),\n",
       "  tensor(2.3039),\n",
       "  tensor(2.2958),\n",
       "  tensor(2.3013),\n",
       "  tensor(2.2946),\n",
       "  tensor(2.3148),\n",
       "  tensor(2.2915),\n",
       "  tensor(2.3086),\n",
       "  tensor(2.2948),\n",
       "  tensor(2.2946),\n",
       "  tensor(2.2677),\n",
       "  tensor(2.3141),\n",
       "  tensor(2.2950),\n",
       "  tensor(2.3000),\n",
       "  tensor(2.3040),\n",
       "  tensor(2.2919),\n",
       "  tensor(2.2887),\n",
       "  tensor(2.3010),\n",
       "  tensor(2.2983),\n",
       "  tensor(2.2878),\n",
       "  tensor(2.2871),\n",
       "  tensor(2.2812),\n",
       "  tensor(2.3008),\n",
       "  tensor(2.3110),\n",
       "  tensor(2.2802),\n",
       "  tensor(2.2901),\n",
       "  tensor(2.2980),\n",
       "  tensor(2.3065),\n",
       "  tensor(2.2963),\n",
       "  tensor(2.3139),\n",
       "  tensor(2.3106),\n",
       "  tensor(2.3033),\n",
       "  tensor(2.3035),\n",
       "  tensor(2.2870),\n",
       "  tensor(2.2800),\n",
       "  tensor(2.2956),\n",
       "  tensor(2.2892),\n",
       "  tensor(2.2938),\n",
       "  tensor(2.3067),\n",
       "  tensor(2.2895),\n",
       "  tensor(2.2956),\n",
       "  tensor(2.2871),\n",
       "  tensor(2.2886),\n",
       "  tensor(2.2841),\n",
       "  tensor(2.2941),\n",
       "  tensor(2.2927),\n",
       "  tensor(2.2931),\n",
       "  tensor(2.2865),\n",
       "  tensor(2.2962),\n",
       "  tensor(2.2911),\n",
       "  tensor(2.2902),\n",
       "  tensor(2.3062),\n",
       "  tensor(2.3045),\n",
       "  tensor(2.2947),\n",
       "  tensor(2.2946),\n",
       "  tensor(2.3033),\n",
       "  tensor(2.2997),\n",
       "  tensor(2.2841),\n",
       "  tensor(2.2949),\n",
       "  tensor(2.2871),\n",
       "  tensor(2.2845),\n",
       "  tensor(2.2960),\n",
       "  tensor(2.2859),\n",
       "  tensor(2.2894),\n",
       "  tensor(2.2924),\n",
       "  tensor(2.2897),\n",
       "  tensor(2.2952),\n",
       "  tensor(2.2837),\n",
       "  tensor(2.2759),\n",
       "  tensor(2.2987),\n",
       "  tensor(2.2821),\n",
       "  tensor(2.2878),\n",
       "  tensor(2.2874),\n",
       "  tensor(2.3017),\n",
       "  tensor(2.2959),\n",
       "  tensor(2.2856),\n",
       "  tensor(2.2848),\n",
       "  tensor(2.2856),\n",
       "  tensor(2.2926),\n",
       "  tensor(2.2818),\n",
       "  tensor(2.3038),\n",
       "  tensor(2.2860),\n",
       "  tensor(2.3005),\n",
       "  tensor(2.2832),\n",
       "  tensor(2.2866),\n",
       "  tensor(2.2756),\n",
       "  tensor(2.2882),\n",
       "  tensor(2.2699),\n",
       "  tensor(2.2838),\n",
       "  tensor(2.2925),\n",
       "  tensor(2.2888),\n",
       "  tensor(2.2727),\n",
       "  tensor(2.2887),\n",
       "  tensor(2.2719),\n",
       "  tensor(2.2881),\n",
       "  tensor(2.2981),\n",
       "  tensor(2.2782),\n",
       "  tensor(2.2964),\n",
       "  tensor(2.2740),\n",
       "  tensor(2.2934),\n",
       "  tensor(2.2887),\n",
       "  tensor(2.2771),\n",
       "  tensor(2.2819),\n",
       "  tensor(2.2711),\n",
       "  tensor(2.2792),\n",
       "  tensor(2.2824),\n",
       "  tensor(2.2741),\n",
       "  tensor(2.2851),\n",
       "  tensor(2.2820),\n",
       "  tensor(2.2752),\n",
       "  tensor(2.2856),\n",
       "  tensor(2.2939),\n",
       "  tensor(2.2823),\n",
       "  tensor(2.2794),\n",
       "  tensor(2.2796),\n",
       "  tensor(2.2799),\n",
       "  tensor(2.2807),\n",
       "  tensor(2.2872),\n",
       "  tensor(2.2684),\n",
       "  tensor(2.2786),\n",
       "  tensor(2.2732),\n",
       "  tensor(2.2702),\n",
       "  tensor(2.2840),\n",
       "  tensor(2.2733),\n",
       "  tensor(2.2863),\n",
       "  tensor(2.2837),\n",
       "  tensor(2.2708),\n",
       "  tensor(2.2765),\n",
       "  tensor(2.2874),\n",
       "  tensor(2.2788),\n",
       "  tensor(2.2913),\n",
       "  tensor(2.2804),\n",
       "  tensor(2.2724),\n",
       "  tensor(2.2915),\n",
       "  tensor(2.2856),\n",
       "  tensor(2.2891),\n",
       "  tensor(2.2753),\n",
       "  tensor(2.2590),\n",
       "  tensor(2.2669),\n",
       "  tensor(2.2693),\n",
       "  tensor(2.2953),\n",
       "  tensor(2.2777),\n",
       "  tensor(2.2882),\n",
       "  tensor(2.2797),\n",
       "  tensor(2.2699),\n",
       "  tensor(2.2971),\n",
       "  tensor(2.2943),\n",
       "  tensor(2.2942),\n",
       "  tensor(2.2729),\n",
       "  tensor(2.2814),\n",
       "  tensor(2.2895),\n",
       "  tensor(2.2893),\n",
       "  tensor(2.2581),\n",
       "  tensor(2.2745),\n",
       "  tensor(2.2685),\n",
       "  tensor(2.2741),\n",
       "  tensor(2.2803),\n",
       "  tensor(2.2738),\n",
       "  tensor(2.2706),\n",
       "  tensor(2.2774),\n",
       "  tensor(2.2849),\n",
       "  tensor(2.2926),\n",
       "  tensor(2.2684),\n",
       "  tensor(2.2749),\n",
       "  tensor(2.2556),\n",
       "  tensor(2.2953),\n",
       "  tensor(2.2799),\n",
       "  tensor(2.2854),\n",
       "  tensor(2.2777),\n",
       "  tensor(2.2980),\n",
       "  tensor(2.2744),\n",
       "  tensor(2.2717),\n",
       "  tensor(2.2592),\n",
       "  tensor(2.2792),\n",
       "  tensor(2.2819),\n",
       "  tensor(2.2781),\n",
       "  tensor(2.2728),\n",
       "  tensor(2.2672),\n",
       "  tensor(2.2864),\n",
       "  tensor(2.2819),\n",
       "  tensor(2.2798),\n",
       "  tensor(2.2729),\n",
       "  tensor(2.2776),\n",
       "  tensor(2.2739),\n",
       "  tensor(2.2695),\n",
       "  tensor(2.2893),\n",
       "  tensor(2.2641),\n",
       "  tensor(2.2706),\n",
       "  tensor(2.2878),\n",
       "  tensor(2.2755),\n",
       "  tensor(2.2724),\n",
       "  tensor(2.2631),\n",
       "  tensor(2.2799),\n",
       "  tensor(2.2773),\n",
       "  tensor(2.2797),\n",
       "  tensor(2.2811),\n",
       "  tensor(2.2654),\n",
       "  tensor(2.2754),\n",
       "  tensor(2.2867),\n",
       "  tensor(2.2634),\n",
       "  tensor(2.2668),\n",
       "  tensor(2.2651),\n",
       "  tensor(2.2717),\n",
       "  tensor(2.2617),\n",
       "  tensor(2.2629),\n",
       "  tensor(2.2676),\n",
       "  tensor(2.2773),\n",
       "  tensor(2.2695),\n",
       "  tensor(2.2628),\n",
       "  tensor(2.2704),\n",
       "  tensor(2.2636),\n",
       "  tensor(2.2756),\n",
       "  tensor(2.2729),\n",
       "  tensor(2.2585),\n",
       "  tensor(2.2743),\n",
       "  tensor(2.2686),\n",
       "  tensor(2.2698),\n",
       "  tensor(2.2687),\n",
       "  tensor(2.2747),\n",
       "  tensor(2.2780),\n",
       "  tensor(2.2562),\n",
       "  tensor(2.2524),\n",
       "  tensor(2.2772),\n",
       "  tensor(2.2749),\n",
       "  tensor(2.2496),\n",
       "  tensor(2.2654),\n",
       "  tensor(2.2810),\n",
       "  tensor(2.2593),\n",
       "  tensor(2.2759),\n",
       "  tensor(2.2650),\n",
       "  tensor(2.2625),\n",
       "  tensor(2.2626),\n",
       "  tensor(2.2708),\n",
       "  tensor(2.2650),\n",
       "  tensor(2.2740),\n",
       "  tensor(2.2676),\n",
       "  tensor(2.2661),\n",
       "  tensor(2.2627),\n",
       "  tensor(2.2755),\n",
       "  tensor(2.2564),\n",
       "  tensor(2.2648),\n",
       "  tensor(2.2742),\n",
       "  tensor(2.2757),\n",
       "  tensor(2.2681),\n",
       "  tensor(2.2694),\n",
       "  tensor(2.2603),\n",
       "  tensor(2.2507),\n",
       "  tensor(2.2535),\n",
       "  tensor(2.2659),\n",
       "  tensor(2.2749),\n",
       "  tensor(2.2484),\n",
       "  tensor(2.2657),\n",
       "  tensor(2.2570),\n",
       "  tensor(2.2644),\n",
       "  tensor(2.2724),\n",
       "  tensor(2.2606),\n",
       "  tensor(2.2594),\n",
       "  tensor(2.2564),\n",
       "  tensor(2.2758),\n",
       "  tensor(2.2573),\n",
       "  tensor(2.2675),\n",
       "  tensor(2.2492),\n",
       "  tensor(2.2664),\n",
       "  tensor(2.2647),\n",
       "  tensor(2.2537),\n",
       "  tensor(2.2551),\n",
       "  tensor(2.2483),\n",
       "  tensor(2.2635),\n",
       "  tensor(2.2742),\n",
       "  tensor(2.2593),\n",
       "  tensor(2.2574),\n",
       "  tensor(2.2547),\n",
       "  tensor(2.2581),\n",
       "  tensor(2.2549),\n",
       "  tensor(2.2491),\n",
       "  tensor(2.2571),\n",
       "  tensor(2.2523),\n",
       "  tensor(2.2634),\n",
       "  tensor(2.2604),\n",
       "  tensor(2.2647),\n",
       "  tensor(2.2550),\n",
       "  tensor(2.2484),\n",
       "  tensor(2.2618),\n",
       "  tensor(2.2426),\n",
       "  tensor(2.2676),\n",
       "  tensor(2.2493),\n",
       "  tensor(2.2460),\n",
       "  tensor(2.2524),\n",
       "  tensor(2.2591),\n",
       "  tensor(2.2441),\n",
       "  tensor(2.2652),\n",
       "  tensor(2.2656),\n",
       "  tensor(2.2512),\n",
       "  tensor(2.2405),\n",
       "  tensor(2.2679),\n",
       "  tensor(2.2455),\n",
       "  tensor(2.2597),\n",
       "  tensor(2.2581),\n",
       "  tensor(2.2473),\n",
       "  tensor(2.2549),\n",
       "  tensor(2.2418),\n",
       "  tensor(2.2565),\n",
       "  tensor(2.2572),\n",
       "  tensor(2.2682),\n",
       "  tensor(2.2640),\n",
       "  tensor(2.2481),\n",
       "  tensor(2.2326),\n",
       "  tensor(2.2506),\n",
       "  tensor(2.2591),\n",
       "  tensor(2.2540),\n",
       "  tensor(2.2605),\n",
       "  tensor(2.2625),\n",
       "  tensor(2.2461),\n",
       "  tensor(2.2414),\n",
       "  tensor(2.2419),\n",
       "  tensor(2.2572),\n",
       "  tensor(2.2638),\n",
       "  tensor(2.2517),\n",
       "  tensor(2.2453),\n",
       "  tensor(2.2420),\n",
       "  tensor(2.2394),\n",
       "  tensor(2.2619),\n",
       "  tensor(2.2503),\n",
       "  tensor(2.2473),\n",
       "  tensor(2.2500),\n",
       "  tensor(2.2478),\n",
       "  tensor(2.2383),\n",
       "  tensor(2.2488),\n",
       "  tensor(2.2493),\n",
       "  tensor(2.2530),\n",
       "  tensor(2.2296),\n",
       "  tensor(2.2403),\n",
       "  tensor(2.2389),\n",
       "  tensor(2.2522),\n",
       "  tensor(2.2338),\n",
       "  tensor(2.2349),\n",
       "  tensor(2.2460),\n",
       "  tensor(2.2564),\n",
       "  tensor(2.2513),\n",
       "  tensor(2.2429),\n",
       "  tensor(2.2381),\n",
       "  tensor(2.2421),\n",
       "  tensor(2.2379),\n",
       "  tensor(2.2501),\n",
       "  tensor(2.2449),\n",
       "  tensor(2.2525),\n",
       "  tensor(2.2361),\n",
       "  tensor(2.2441),\n",
       "  tensor(2.2588),\n",
       "  tensor(2.2544),\n",
       "  tensor(2.2225),\n",
       "  tensor(2.2357),\n",
       "  tensor(2.2465),\n",
       "  tensor(2.2336),\n",
       "  tensor(2.2380),\n",
       "  tensor(2.2233),\n",
       "  tensor(2.2407),\n",
       "  tensor(2.2380),\n",
       "  tensor(2.2246),\n",
       "  tensor(2.2396),\n",
       "  tensor(2.2172),\n",
       "  tensor(2.2120),\n",
       "  tensor(2.2232),\n",
       "  tensor(2.2210),\n",
       "  tensor(2.2329),\n",
       "  tensor(2.2290),\n",
       "  tensor(2.2340),\n",
       "  tensor(2.2513),\n",
       "  tensor(2.2229),\n",
       "  tensor(2.2238),\n",
       "  tensor(2.2240),\n",
       "  tensor(2.2245),\n",
       "  tensor(2.2231),\n",
       "  tensor(2.2360),\n",
       "  tensor(2.2312),\n",
       "  tensor(2.2367),\n",
       "  tensor(2.2275),\n",
       "  tensor(2.2195),\n",
       "  tensor(2.2498),\n",
       "  tensor(2.2330),\n",
       "  tensor(2.2358),\n",
       "  tensor(2.2289),\n",
       "  tensor(2.2302),\n",
       "  tensor(2.2043),\n",
       "  tensor(2.2002),\n",
       "  tensor(2.2228),\n",
       "  tensor(2.2281),\n",
       "  tensor(2.2249),\n",
       "  tensor(2.2325),\n",
       "  tensor(2.2209),\n",
       "  tensor(2.2239),\n",
       "  tensor(2.2244),\n",
       "  tensor(2.2286),\n",
       "  tensor(2.2047),\n",
       "  tensor(2.2195),\n",
       "  tensor(2.2250),\n",
       "  tensor(2.2182),\n",
       "  tensor(2.2296),\n",
       "  tensor(2.2233),\n",
       "  tensor(2.2394),\n",
       "  tensor(2.2296),\n",
       "  tensor(2.2233),\n",
       "  tensor(2.2358),\n",
       "  tensor(2.2042),\n",
       "  tensor(2.2223),\n",
       "  tensor(2.2061),\n",
       "  tensor(2.2065),\n",
       "  tensor(2.2265),\n",
       "  tensor(2.2245),\n",
       "  tensor(2.2429),\n",
       "  tensor(2.2054),\n",
       "  tensor(2.2049),\n",
       "  tensor(2.2154),\n",
       "  tensor(2.2066),\n",
       "  tensor(2.2260),\n",
       "  tensor(2.2268),\n",
       "  tensor(2.2173),\n",
       "  tensor(2.2006),\n",
       "  tensor(2.1994),\n",
       "  tensor(2.2308),\n",
       "  tensor(2.2108),\n",
       "  tensor(2.2086),\n",
       "  tensor(2.1989),\n",
       "  tensor(2.2137),\n",
       "  tensor(2.2067),\n",
       "  tensor(2.2130),\n",
       "  tensor(2.2111),\n",
       "  tensor(2.2170),\n",
       "  tensor(2.2299),\n",
       "  tensor(2.2043),\n",
       "  tensor(2.2040),\n",
       "  tensor(2.2150),\n",
       "  tensor(2.2172),\n",
       "  tensor(2.2082),\n",
       "  tensor(2.2114),\n",
       "  tensor(2.2063),\n",
       "  tensor(2.1989),\n",
       "  tensor(2.2110),\n",
       "  tensor(2.2030),\n",
       "  tensor(2.2005),\n",
       "  tensor(2.2011),\n",
       "  tensor(2.2151),\n",
       "  tensor(2.2135),\n",
       "  tensor(2.2078),\n",
       "  tensor(2.2095),\n",
       "  tensor(2.1937),\n",
       "  tensor(2.1752),\n",
       "  tensor(2.1700),\n",
       "  tensor(2.1938),\n",
       "  tensor(2.1932),\n",
       "  tensor(2.2096),\n",
       "  tensor(2.2092),\n",
       "  tensor(2.2084),\n",
       "  tensor(2.2027),\n",
       "  tensor(2.1826),\n",
       "  tensor(2.1865),\n",
       "  tensor(2.1971),\n",
       "  tensor(2.1788),\n",
       "  tensor(2.1673),\n",
       "  tensor(2.1994),\n",
       "  tensor(2.1805),\n",
       "  tensor(2.1841),\n",
       "  tensor(2.1887),\n",
       "  tensor(2.2036),\n",
       "  tensor(2.1895),\n",
       "  tensor(2.2086),\n",
       "  tensor(2.1966),\n",
       "  tensor(2.1897),\n",
       "  tensor(2.1930),\n",
       "  tensor(2.1812),\n",
       "  tensor(2.1922),\n",
       "  tensor(2.1632),\n",
       "  tensor(2.1675),\n",
       "  tensor(2.1911),\n",
       "  tensor(2.1516),\n",
       "  tensor(2.1947),\n",
       "  tensor(2.1836),\n",
       "  tensor(2.1829),\n",
       "  tensor(2.1610),\n",
       "  tensor(2.1794),\n",
       "  tensor(2.1700),\n",
       "  tensor(2.1795),\n",
       "  tensor(2.1785),\n",
       "  tensor(2.1754),\n",
       "  tensor(2.1639),\n",
       "  tensor(2.1825),\n",
       "  tensor(2.1683),\n",
       "  tensor(2.1845),\n",
       "  tensor(2.1503),\n",
       "  tensor(2.1560),\n",
       "  tensor(2.1720),\n",
       "  tensor(2.1796),\n",
       "  tensor(2.1802),\n",
       "  tensor(2.1558),\n",
       "  tensor(2.1845),\n",
       "  tensor(2.2073),\n",
       "  tensor(2.1578),\n",
       "  tensor(2.1586),\n",
       "  tensor(2.1568),\n",
       "  tensor(2.1466),\n",
       "  tensor(2.1665),\n",
       "  tensor(2.1532),\n",
       "  tensor(2.1556),\n",
       "  tensor(2.1828),\n",
       "  tensor(2.1424),\n",
       "  tensor(2.1585),\n",
       "  tensor(2.1603),\n",
       "  tensor(2.1612),\n",
       "  tensor(2.1510),\n",
       "  tensor(2.1740),\n",
       "  tensor(2.1401),\n",
       "  tensor(2.1486),\n",
       "  tensor(2.1608),\n",
       "  tensor(2.1360),\n",
       "  tensor(2.1535),\n",
       "  tensor(2.1539),\n",
       "  tensor(2.1745),\n",
       "  tensor(2.1440),\n",
       "  tensor(2.1469),\n",
       "  tensor(2.1556),\n",
       "  tensor(2.1505),\n",
       "  tensor(2.1370),\n",
       "  tensor(2.1282),\n",
       "  tensor(2.1213),\n",
       "  tensor(2.1255),\n",
       "  tensor(2.1574),\n",
       "  tensor(2.1663),\n",
       "  tensor(2.1473),\n",
       "  tensor(2.1494),\n",
       "  tensor(2.1561),\n",
       "  tensor(2.1172),\n",
       "  tensor(2.0870),\n",
       "  tensor(2.1250),\n",
       "  tensor(2.1333),\n",
       "  tensor(2.1019),\n",
       "  tensor(2.1305),\n",
       "  tensor(2.1294),\n",
       "  tensor(2.1462),\n",
       "  tensor(2.1289),\n",
       "  tensor(2.1072),\n",
       "  tensor(2.0884),\n",
       "  tensor(2.1390),\n",
       "  tensor(2.1244),\n",
       "  tensor(2.0870),\n",
       "  tensor(2.1107),\n",
       "  tensor(2.1649),\n",
       "  tensor(2.1222),\n",
       "  tensor(2.1230),\n",
       "  tensor(2.1193),\n",
       "  tensor(2.1054),\n",
       "  tensor(2.0913),\n",
       "  tensor(2.0998),\n",
       "  tensor(2.1181),\n",
       "  tensor(2.0689),\n",
       "  tensor(2.1019),\n",
       "  tensor(2.0913),\n",
       "  tensor(2.1198),\n",
       "  tensor(2.1626),\n",
       "  tensor(2.1236),\n",
       "  tensor(2.0717),\n",
       "  tensor(2.1155),\n",
       "  tensor(2.1329),\n",
       "  tensor(2.0779),\n",
       "  tensor(2.0719),\n",
       "  tensor(2.0793),\n",
       "  tensor(2.0613),\n",
       "  tensor(2.0754),\n",
       "  tensor(2.0943),\n",
       "  tensor(2.0756),\n",
       "  tensor(2.0745),\n",
       "  tensor(2.0913),\n",
       "  tensor(2.1248),\n",
       "  tensor(2.0944),\n",
       "  tensor(2.0680),\n",
       "  tensor(2.1037),\n",
       "  tensor(2.1066),\n",
       "  tensor(2.0973),\n",
       "  tensor(2.0906),\n",
       "  tensor(2.0624),\n",
       "  tensor(2.1214),\n",
       "  tensor(2.0392),\n",
       "  tensor(2.0430),\n",
       "  tensor(2.1048),\n",
       "  tensor(2.0824),\n",
       "  tensor(2.0370),\n",
       "  tensor(2.0259),\n",
       "  tensor(2.0767),\n",
       "  tensor(2.0534),\n",
       "  tensor(2.0708),\n",
       "  tensor(2.0313),\n",
       "  tensor(2.0342),\n",
       "  tensor(2.0609),\n",
       "  tensor(2.0507),\n",
       "  tensor(2.0403),\n",
       "  tensor(2.0508),\n",
       "  tensor(2.0500),\n",
       "  tensor(2.0720),\n",
       "  tensor(2.0535),\n",
       "  tensor(2.0547),\n",
       "  tensor(2.0336),\n",
       "  tensor(2.0555),\n",
       "  tensor(1.9790),\n",
       "  tensor(2.0249),\n",
       "  tensor(2.0329),\n",
       "  tensor(2.0209),\n",
       "  tensor(2.0470),\n",
       "  tensor(1.9917),\n",
       "  tensor(2.0486),\n",
       "  tensor(2.0238),\n",
       "  tensor(2.0389),\n",
       "  tensor(1.9635),\n",
       "  tensor(1.9716),\n",
       "  tensor(1.9909),\n",
       "  tensor(2.0003),\n",
       "  tensor(2.0028),\n",
       "  tensor(2.0280),\n",
       "  tensor(2.0126),\n",
       "  tensor(1.9855),\n",
       "  tensor(2.0221),\n",
       "  tensor(2.0240),\n",
       "  tensor(1.9798),\n",
       "  tensor(1.9614),\n",
       "  tensor(1.9600),\n",
       "  tensor(1.9887),\n",
       "  tensor(1.9744),\n",
       "  tensor(1.9613),\n",
       "  tensor(2.0241),\n",
       "  tensor(2.0186),\n",
       "  tensor(1.9476),\n",
       "  tensor(1.9560),\n",
       "  tensor(2.0042),\n",
       "  tensor(1.9314),\n",
       "  tensor(1.9811),\n",
       "  tensor(1.9590),\n",
       "  tensor(1.9693),\n",
       "  tensor(1.9442),\n",
       "  tensor(1.9703),\n",
       "  tensor(1.9161),\n",
       "  tensor(1.9117),\n",
       "  tensor(1.9490),\n",
       "  tensor(1.9339),\n",
       "  tensor(1.9443),\n",
       "  tensor(1.9556),\n",
       "  tensor(1.9701),\n",
       "  tensor(1.8813),\n",
       "  tensor(1.9117),\n",
       "  tensor(1.9264),\n",
       "  tensor(1.9412),\n",
       "  tensor(1.8864),\n",
       "  tensor(1.9086),\n",
       "  tensor(1.9310),\n",
       "  tensor(1.9023),\n",
       "  tensor(1.8944),\n",
       "  tensor(1.9617),\n",
       "  tensor(1.8217),\n",
       "  tensor(1.9452),\n",
       "  tensor(1.8938),\n",
       "  tensor(1.8492),\n",
       "  tensor(1.8676),\n",
       "  tensor(1.8770),\n",
       "  tensor(1.8399),\n",
       "  tensor(1.9150),\n",
       "  tensor(1.8546),\n",
       "  tensor(1.8406),\n",
       "  tensor(1.9091),\n",
       "  tensor(1.8653),\n",
       "  tensor(1.8187),\n",
       "  tensor(1.8878),\n",
       "  tensor(1.9002),\n",
       "  tensor(1.7791),\n",
       "  tensor(1.8989),\n",
       "  tensor(1.8167),\n",
       "  tensor(1.8464),\n",
       "  tensor(1.8579),\n",
       "  tensor(1.8845),\n",
       "  tensor(1.7935),\n",
       "  tensor(1.7654),\n",
       "  tensor(1.7934),\n",
       "  tensor(1.8522),\n",
       "  tensor(1.8416),\n",
       "  tensor(1.7694),\n",
       "  tensor(1.7886),\n",
       "  tensor(1.7813),\n",
       "  tensor(1.8995),\n",
       "  tensor(1.7982),\n",
       "  tensor(1.6833),\n",
       "  tensor(1.7220),\n",
       "  tensor(1.7788),\n",
       "  tensor(1.8061),\n",
       "  tensor(1.7408),\n",
       "  tensor(1.8076),\n",
       "  tensor(1.7334),\n",
       "  tensor(1.7738),\n",
       "  tensor(1.7111),\n",
       "  tensor(1.7189),\n",
       "  tensor(1.7186),\n",
       "  tensor(1.7784),\n",
       "  tensor(1.7402),\n",
       "  tensor(1.6835),\n",
       "  tensor(1.7322),\n",
       "  tensor(1.7049),\n",
       "  tensor(1.7205),\n",
       "  tensor(1.7803),\n",
       "  tensor(1.6773),\n",
       "  tensor(1.6685),\n",
       "  tensor(1.6894),\n",
       "  tensor(1.6937),\n",
       "  tensor(1.6908),\n",
       "  tensor(1.7562),\n",
       "  tensor(1.7383),\n",
       "  tensor(1.6679),\n",
       "  tensor(1.6818),\n",
       "  tensor(1.7454),\n",
       "  tensor(1.6811),\n",
       "  tensor(1.6552),\n",
       "  tensor(1.6297),\n",
       "  tensor(1.6486),\n",
       "  tensor(1.5896),\n",
       "  tensor(1.7609),\n",
       "  tensor(1.6580),\n",
       "  tensor(1.6800),\n",
       "  tensor(1.6672),\n",
       "  tensor(1.6288),\n",
       "  tensor(1.8129),\n",
       "  tensor(1.5815),\n",
       "  tensor(1.5675),\n",
       "  tensor(1.6229),\n",
       "  tensor(1.5829),\n",
       "  tensor(1.5863),\n",
       "  tensor(1.5958),\n",
       "  tensor(1.6302),\n",
       "  tensor(1.7140),\n",
       "  tensor(1.6566),\n",
       "  tensor(1.5775),\n",
       "  tensor(1.5794),\n",
       "  tensor(1.7055),\n",
       "  tensor(1.5132),\n",
       "  tensor(1.5659),\n",
       "  tensor(1.5047),\n",
       "  tensor(1.5187),\n",
       "  tensor(1.6279),\n",
       "  tensor(1.6262),\n",
       "  tensor(1.4091),\n",
       "  tensor(1.4940),\n",
       "  tensor(1.4890),\n",
       "  tensor(1.6051),\n",
       "  tensor(1.5613),\n",
       "  tensor(1.5172),\n",
       "  tensor(1.5191),\n",
       "  tensor(1.5187),\n",
       "  tensor(1.5509),\n",
       "  tensor(1.4210),\n",
       "  tensor(1.3889),\n",
       "  tensor(1.4369),\n",
       "  tensor(1.5535),\n",
       "  tensor(1.3710),\n",
       "  tensor(1.3104),\n",
       "  tensor(1.4888),\n",
       "  tensor(1.3938),\n",
       "  tensor(1.3658),\n",
       "  tensor(1.4081),\n",
       "  tensor(1.5140),\n",
       "  tensor(1.4834),\n",
       "  tensor(1.5327),\n",
       "  tensor(1.4265),\n",
       "  tensor(1.3449),\n",
       "  tensor(1.4928),\n",
       "  tensor(1.2633),\n",
       "  tensor(1.4481),\n",
       "  tensor(1.3432),\n",
       "  tensor(1.3785),\n",
       "  tensor(1.4856),\n",
       "  tensor(1.3444),\n",
       "  tensor(1.4680),\n",
       "  tensor(1.4600),\n",
       "  tensor(1.3097),\n",
       "  tensor(1.3222),\n",
       "  tensor(1.4654),\n",
       "  tensor(1.4607),\n",
       "  tensor(1.3324),\n",
       "  tensor(1.2712),\n",
       "  tensor(1.0840),\n",
       "  tensor(1.2946),\n",
       "  tensor(1.3247),\n",
       "  tensor(1.4096),\n",
       "  tensor(1.2881),\n",
       "  tensor(1.2285),\n",
       "  tensor(1.1509),\n",
       "  tensor(1.2391),\n",
       "  tensor(1.2136),\n",
       "  tensor(1.3288),\n",
       "  tensor(1.3484),\n",
       "  tensor(1.2610),\n",
       "  tensor(1.1399),\n",
       "  tensor(1.1669),\n",
       "  tensor(1.2384),\n",
       "  tensor(1.3208),\n",
       "  tensor(1.2816),\n",
       "  tensor(1.2844),\n",
       "  tensor(1.2658),\n",
       "  tensor(1.2045),\n",
       "  tensor(1.2352),\n",
       "  tensor(1.1474),\n",
       "  tensor(1.2445),\n",
       "  tensor(1.2692),\n",
       "  tensor(1.1942),\n",
       "  tensor(1.3240),\n",
       "  tensor(1.2695),\n",
       "  tensor(1.1257),\n",
       "  tensor(1.1760),\n",
       "  tensor(1.2903),\n",
       "  tensor(1.1643),\n",
       "  tensor(1.2076),\n",
       "  tensor(1.1819),\n",
       "  tensor(1.0557),\n",
       "  tensor(1.0969),\n",
       "  tensor(0.9645),\n",
       "  tensor(1.0195),\n",
       "  tensor(1.1151),\n",
       "  tensor(1.2007),\n",
       "  tensor(1.1422),\n",
       "  tensor(1.1657),\n",
       "  tensor(1.1041),\n",
       "  tensor(1.1709),\n",
       "  tensor(1.0198),\n",
       "  tensor(1.1444),\n",
       "  tensor(1.1629),\n",
       "  tensor(1.0582),\n",
       "  tensor(1.1634),\n",
       "  tensor(1.0525),\n",
       "  tensor(1.2050),\n",
       "  tensor(1.2821),\n",
       "  tensor(0.9592),\n",
       "  tensor(1.1821),\n",
       "  tensor(1.2209),\n",
       "  tensor(1.0562),\n",
       "  tensor(1.1125),\n",
       "  tensor(1.1664),\n",
       "  tensor(1.0915),\n",
       "  tensor(0.9795),\n",
       "  tensor(1.0322),\n",
       "  tensor(0.9437),\n",
       "  tensor(1.1464),\n",
       "  tensor(0.9042),\n",
       "  tensor(0.9990),\n",
       "  tensor(1.0913),\n",
       "  tensor(0.9326),\n",
       "  tensor(1.1464),\n",
       "  tensor(1.0826),\n",
       "  tensor(1.0935),\n",
       "  tensor(1.1485),\n",
       "  tensor(1.0488),\n",
       "  tensor(1.0463),\n",
       "  tensor(0.8660),\n",
       "  tensor(1.0750),\n",
       "  tensor(0.8201),\n",
       "  tensor(1.0772),\n",
       "  tensor(0.8801),\n",
       "  tensor(0.9192),\n",
       "  tensor(0.9972),\n",
       "  tensor(0.9276),\n",
       "  tensor(1.0664),\n",
       "  tensor(0.9874),\n",
       "  tensor(0.9387),\n",
       "  tensor(1.1347),\n",
       "  tensor(0.9751),\n",
       "  tensor(0.9373),\n",
       "  tensor(1.0690),\n",
       "  tensor(0.9341),\n",
       "  tensor(0.9775),\n",
       "  tensor(0.9369),\n",
       "  tensor(0.8834),\n",
       "  tensor(0.9253),\n",
       "  tensor(0.8676),\n",
       "  tensor(0.7770),\n",
       "  tensor(0.9045),\n",
       "  tensor(0.8156),\n",
       "  tensor(1.0989),\n",
       "  tensor(0.9932),\n",
       "  tensor(0.9817),\n",
       "  tensor(0.9648),\n",
       "  tensor(0.8885),\n",
       "  tensor(0.8920),\n",
       "  tensor(0.9414),\n",
       "  tensor(0.9748),\n",
       "  tensor(0.8802),\n",
       "  tensor(1.0192),\n",
       "  tensor(0.8659),\n",
       "  tensor(0.9291),\n",
       "  tensor(0.8911),\n",
       "  tensor(0.9672),\n",
       "  tensor(0.9343),\n",
       "  tensor(0.9294),\n",
       "  tensor(1.0397),\n",
       "  tensor(0.7123),\n",
       "  tensor(1.0303),\n",
       "  tensor(0.9908),\n",
       "  tensor(0.8235),\n",
       "  tensor(0.8301),\n",
       "  tensor(0.9187),\n",
       "  tensor(0.8132),\n",
       "  tensor(0.8875),\n",
       "  tensor(0.8573),\n",
       "  tensor(0.7763),\n",
       "  tensor(0.9748),\n",
       "  tensor(0.8381),\n",
       "  tensor(1.1174),\n",
       "  tensor(0.9020),\n",
       "  tensor(0.7278),\n",
       "  tensor(0.8256),\n",
       "  tensor(0.8309),\n",
       "  tensor(0.8551),\n",
       "  tensor(0.9463),\n",
       "  tensor(0.7108),\n",
       "  tensor(0.8564),\n",
       "  tensor(0.8331),\n",
       "  tensor(0.7351),\n",
       "  tensor(0.6940),\n",
       "  tensor(0.7983),\n",
       "  tensor(1.0076),\n",
       "  tensor(0.8335),\n",
       "  tensor(0.9401),\n",
       "  tensor(0.7009),\n",
       "  tensor(0.8382),\n",
       "  tensor(0.8048),\n",
       "  tensor(0.8940),\n",
       "  tensor(0.7654),\n",
       "  tensor(0.7635),\n",
       "  tensor(0.7532),\n",
       "  tensor(0.8141),\n",
       "  tensor(0.6394),\n",
       "  tensor(0.7530),\n",
       "  tensor(0.8029),\n",
       "  tensor(0.9783),\n",
       "  tensor(0.6749),\n",
       "  tensor(0.7573),\n",
       "  tensor(0.7911),\n",
       "  tensor(0.8710),\n",
       "  tensor(0.7349),\n",
       "  tensor(0.6753),\n",
       "  tensor(0.7653),\n",
       "  tensor(0.9061),\n",
       "  tensor(0.7515),\n",
       "  tensor(1.0831),\n",
       "  tensor(0.7936),\n",
       "  tensor(0.9906),\n",
       "  tensor(0.9881),\n",
       "  ...])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyMNISTNet = MNISTNet()\n",
    "#MyMNISTNet.cuda() #<= if cuda\n",
    "\n",
    "MNISTOptimizer = createOptimizer(MyMNISTNet)\n",
    "\n",
    "train(MyMNISTNet, train_loader, MNISTOptimizer, count_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [10, 1, 5, 5], but got 2-dimensional input of size [28, 28] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7e5e734d05bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#print(Variable(evaluate_x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyMNISTNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-6aed344e8e69>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#conv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [10, 1, 5, 5], but got 2-dimensional input of size [28, 28] instead"
     ]
    }
   ],
   "source": [
    "#evaluate_x = Variable(test_loader.dataset.test_data.type_as(torch.FloatTensor()))\n",
    "#evaluate_y = Variable(test_loader.dataset.test_labels)\n",
    "\n",
    "evaluate_x = test_loader.dataset.data[0]\n",
    "evaluate_x = Variable(evaluate_x)\n",
    "#print(evaluate_x)\n",
    "\n",
    "#evaluate_y = Variable(test_loader.dataset.test_labels[0])\n",
    "\n",
    "#print(Variable(evaluate_x))\n",
    "output = MyMNISTNet(evaluate_x)\n",
    "\n",
    "#print(output)\n",
    "\n",
    "#pred = output.data.max(1)[1]\n",
    "#d = pred.eq(evaluate_y.data).cpu()\n",
    "#accuracy = d.sum()/d.size()[0]\n",
    "\n",
    "#print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAAxtJREFUeJztmz1LI1EUhp9ZVrQI8YMsSMBsQEEREUEbbQQVieIPEPwHFnbaKUgaQbFSUPQnmEIQsREELQIKYmMXktgIrh+FhSDK3SJrsp4UmZHczCx7HgiBy2XuyZP3JHMnE8cYg1Lim98FBA0VIlAhAhUiUCECFSJQIQIVIlAhgu9eJkciEROPxy2VYpdcLsf9/b1TaZ4nIfF4nIuLi69X5SMDAwOu5mnLCFSIQIUIVIhAhQhUiECFCFSIQIUIPJ2pumVvbw+AnZ0dAKLRKA0NDQDMzMwA0NraCkBHR4eNEr6MJkRgJSHz8/NAYUMl2draAiAcDgPQ3d3t+fhtbW0ALCwsAO73KW6wImR3dxeAq6sroPCir6+vAbi8vATg5OQEgHQ6TSwWA+Dm5qbsWHV1dQBEIhEAbm9vSafTQElMNYVoywisJGR0dPTTM0Aikfg05+npCSgk5uMdPj8/LztWfX09AJ2dnQB0dXXx+PgIQHt7e5Ur14SUYSUhbmhubgZgZGSkOPZ3oiSpVAooJKu3txeA6enpqtelCRH4lhC33N3dATA7OwuAMYalpSUAWlpaqr5e4IVsbm4CJTFNTU3FD1gbaMsIApuQs7MzAFZWVj6N7+/v09PTY21dTYggsAk5PDwE4PX1FYCxsTEABgcHra4bSCEvLy8cHR0BpTPV5eVloLS3sYW2jCCQCVldXS3uiicmJgAYGhqqydqaEEGgEnJwcABAMpmksbERgMXFxZrWoAkRBCIhDw8PAMzNzQHw9vbG5OQkYP9rVuK7kPf39+LFo2w2CxSuxCeTSV/q0ZYR+J6QTCZTdlfS+vq6lcuDbtCECHxLSD6fB2B8fLw4tra2BsDU1JQvNYGPQra3t4GSGIDh4WEAHKfizYLW0JYR1Dwhp6enAGxsbNR6aVdoQgQ1T8jHpcHn5+fi2MctEaFQqNbllKEJEfh+YtbX18fx8TFg53cWzxhjXD/6+/vNv8qf2iu+Rm0ZgWM8/JHZcZxfQL7ixGDy0xjzo9IkT0L+B7RlBCpEoEIEKkSgQgQqRKBCBCpEoEIEvwEKw6jrEKgkZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA11JREFUeJztmz1LI1EUhp/o2ogiSEKwMQEbBRFBIQiCliIqouAUIhZW/gURCxHx20rwB1j4hYrYCNYWEhAtFCSIW4lsgliJWswWbibkTPyYxbnjsueBEDj3hDnz5j137p1JQrZto+QoCrqA74YKIlBBBCqIQAURqCACFUSggghUEMEPL8nhcNiOx+M+leIvNzc3pNPp0Ed5ngSJx+Mkk8m/rypAmpubP5WnLSNQQQSeWuYrWFhYAODx8RGA8/Nztre383JGR0dpaWkBYGhoyGh96hCBMYdYlgXA1taWaywUyp/8V1dXOTo6AqCtrQ2A6upqnyt8RR0iMOIQy7IKOgOgtraWjo4OAK6vrwHY398nlUoBsLa2BsDY2JiBSn0WJLtm2d3ddWL19fXA60kDhMNhysrKAHh+fgYgkUhwdnYGQCaT8bNEF9oyAl8dcnt7C4Bt244zDg8PAaiqqnLlZy/Jl5eXTqyrq8vPEl2oQwS+OqS7uxuAVCpFeXk5AJWVlW/mb2xsALm5JAiMXGVisdi74/Pz8wBcXV05sUQikfduCm0ZgfG9jOTg4ICJiQkAnp6eAIhGo8zMzABQWlpqtB51iCBwhySTSccZWSzLcvYwplGHCAJzSG9vL5BbqAEMDw8DMDU1FUhNEIAg2dXr8fEx8DqRRiIRAMbHxwGcvU0QaMsIjDukr68PgHQ67cQGBwcBqKmpMV2OC3WIwJhDsvc/Tk9P8+Lt7e1MTk6aKuNDjAiSyWSYnp4G3Bu3xsbGQCdRibaMwIhDFhcXOTk5yYtl1yHfqV1AHeLCiEOWlpZcsZWVFSDYRVgh1CGCwPYy2ccLJSUlBccrKiryxl9eXgB4eHhwcu7v7wFYXl52fb64uBiA2dlZT/dUAhOkoaHh3fGBgQEgd3f+7u4OgPX1dU/HiUajzh7pM2jLCIw4pLOzk729PU+f2dzcfHMs20ZFRbnvs6enB3D/Uqi1tdXTcdUhAiMO2dnZYW5uDij8zOXi4gIoPD+MjIwA+Y8y+vv7Aairq/vyWrFt+9OvpqYm+1/lT+0fnqO2jEAFEaggAhVEoIIIVBCBCiJQQQQh28MfmUOh0C/gp3/l+ErMtu3IR0meBPkf0JYRqCACFUSggghUEIEKIlBBBCqIQAUR/AZTesie1UWqVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(test_loader.dataset.test_data)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images_separately(images):\n",
    "    \"Plot the six MNIST images separately.\"\n",
    "    fig = plt.figure()\n",
    "    for j in range(1, 7):\n",
    "        ax = fig.add_subplot(1, 6, j)\n",
    "        ax.matshow(images[j-1], cmap = matplotlib.cm.binary)\n",
    "        plt.xticks(np.array([]))\n",
    "        plt.yticks(np.array([]))\n",
    "    plt.show()\n",
    "\n",
    "def plot_image(image):\n",
    "    \"Plot a MNIST image.\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(4, 1, 1)\n",
    "    ax.matshow(image, cmap = matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "    plt.show()\n",
    "    \n",
    "plot_images_separately(test_loader.dataset.test_data)\n",
    "plot_images_separately([\n",
    "    test_loader.dataset.test_data[0],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1]\n",
    "])\n",
    "plot_image(test_loader.dataset.test_data[0])\n",
    "plot_image(test_loader.dataset.test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
